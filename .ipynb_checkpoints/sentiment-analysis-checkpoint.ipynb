{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RajeevAtla/HTCS-ML-Team-B/blob/master/sentiment-analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEph_rmi9lCm"
   },
   "source": [
    "# Building a Sentiment Classifier using Scikit-Learn\n",
    "\n",
    "Acknowledgement: This is derived from https://towardsdatascience.com/building-a-sentiment-classifier-using-scikit-learn-54c8e7c5d2f0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "blTQmk0V9lCn"
   },
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/lazuxd/simple-imdb-sentiment-analysis/master/smiley.jpg\"/></center>\n",
    "<center><i>Image by AbsolutVision @ <a href=\"https://pixabay.com/ro/photos/smiley-emoticon-furie-sup%C4%83rat-2979107/\">pixabay.com</a></i></center>\n",
    "\n",
    "> &nbsp;&nbsp;&nbsp;&nbsp;**Sentiment analysis**, an important area in Natural Language Processing, is the process of automatically detecting affective states of text. Sentiment analysis is widely applied to voice-of-customer materials such as product reviews in online shopping websites like Amazon, movie reviews or social media. It can be just a basic task of classifying the polarity of a text as being positive/negative or it can go beyond polarity, looking at emotional states such as \"happy\", \"angry\", etc.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Here we will build a classifier that is able to distinguish movie reviews as being either positive or negative. For that, we will use [Large Movie Review Dataset v1.0](http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz)<sup>(2)</sup> of IMDB movie reviews.\n",
    "This dataset contains 50,000 movie reviews divided evenly into 25k train and 25k test. The labels are balanced between the two classes (positive and negative). Reviews with a score <= 4 out of 10 are labeled negative and those with score >= 7 out of 10 are labeled positive. Neutral reviews are not included in the labeled data. This dataset also contains unlabeled reviews for unsupervised learning; we will not use them here. There are no more than 30 reviews for a particular movie because the ratings of the same movie tend to be correlated. All reviews for a given movie are either in train or test set but not in both, in order to avoid test accuracy gain by memorizing movie-specific terms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7CL4d-HH9lCn"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evm9tjsW9lCo"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;After the dataset has been downloaded and extracted from archive we have to transform it into a more suitable form for feeding it into a machine learning model for training. We will start by combining all review data into 2 pandas Data Frames representing the train and test datasets, and then saving them as csv files: *imdb_train.csv* and *imdb_test.csv*.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The Data Frames will have the following form:  \n",
    "\n",
    "|text       |label      |\n",
    "|:---------:|:---------:|\n",
    "|review1    |0          |\n",
    "|review2    |1          |\n",
    "|review3    |1          |\n",
    "|.......    |...        |\n",
    "|reviewN    |0          |  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;where:  \n",
    "- review1, review2, ... = the actual text of movie review  \n",
    "- 0 = negative review  \n",
    "- 1 = positive review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EckG-g0E9lCo"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;But machine learnng algorithms work only with numerical values. We can't just input the text itself into a machine learning model and have it learn from that. We have to, somehow, represent the text by numbers or vectors of numbers. One way of doing this is by using the **Bag-of-words** model<sup>(3)</sup>, in which a piece of text(often called a **document**) is represented by a vector of the counts of words from a vocabulary in that document. This model doesn't take into account grammar rules or word ordering; all it considers is the frequency of words. If we use the counts of each word independently we name this representation a **unigram**. In general, in a **n-gram** we take into account the counts of each combination of n words from the vocabulary that appears in a given document.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;For example, consider these two documents:  \n",
    "<br>  \n",
    "<div style=\"font-family: monospace;\"><center><b>d1: \"I am learning\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</b></center></div>  \n",
    "<div style=\"font-family: monospace;\"><center><b>d2: \"Machine learning is cool\"</b></center></div>  \n",
    "<br>\n",
    "The vocabulary of all words encountered in these two sentences is: \n",
    "\n",
    "<br/>  \n",
    "<div style=\"font-family: monospace;\"><center><b>v: [ I, am, learning, machine, is, cool ]</b></center></div>   \n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The unigram representations of d1 and d2:  \n",
    "<br>  \n",
    "\n",
    "|unigram(d1)|I       |am      |learning|machine |is      |cool    |\n",
    "|:---------:|:------:|:------:|:------:|:------:|:------:|:------:|\n",
    "|           |1       |1       |1       |0       |0       |0       |  \n",
    "\n",
    "|unigram(d2)|I       |am      |learning|machine |is      |cool    |\n",
    "|:---------:|:------:|:------:|:------:|:------:|:------:|:------:|\n",
    "|           |0       |0       |1       |1       |1       |1       |\n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;And, the bigrams of d1 and d2 are:\n",
    "  \n",
    "|bigram(d1) |I I     |I am    |I learning|...|machine am|machine learning|...|cool is|cool cool|\n",
    "|:---------:|:------:|:------:|:--------:|:-:|:--------:|:--------------:|:-:|:-----:|:-------:|\n",
    "|           |0       |1       |0         |...|0         |0               |...|0      |0        |  \n",
    "\n",
    "|bigram(d2) |I I     |I am    |I learning|...|machine am|machine learning|...|cool is|cool cool|\n",
    "|:---------:|:------:|:------:|:--------:|:-:|:--------:|:--------------:|:-:|:-----:|:-------:|\n",
    "|           |0       |0       |0         |...|0         |1               |...|0      |0        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rcg3cjFI9lCo"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Often, we can achieve slightly better results if instead of counts of words we use something called **term frequency times inverse document frequency** (or **tf-idf**). Maybe it sounds complicated, but it is not. Bear with me, I will explain this. The intuition behind this is the following. So, what's the problem of using just the frequency of terms inside a document? Although some terms may have a high frequency inside documents they may not be so relevant for describing a given document in which they appear. That's because those terms may also have a high frequency across the collection of all documents. For example, a collection of movie reviews may have terms specific to movies/cinematography that are present in almost all documents(they have a high **document frequency**). So, when we encounter those terms in a document this doesn't tell much about whether it is a positive or negative review. We need a way of relating **term frequency** (how frequent a term is inside a document) to **document frequency** (how frequent a term is across the whole collection of documents). That is:  \n",
    "  \n",
    "$$\\begin{align}\\frac{\\text{term frequency}}{\\text{document frequency}} &= \\text{term frequency} \\cdot \\frac{1}{\\text{document frequency}} \\\\ &= \\text{term frequency} \\cdot \\text{inverse document frequency} \\\\ &= \\text{tf} \\cdot \\text{idf}\\end{align}$$  \n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Now, there are more ways used to describe both term frequency and inverse document frequency. But the most common way is by putting them on a logarithmic scale:  \n",
    "  \n",
    "$$tf(t, d) = log(1+f_{t,d})$$  \n",
    "$$idf(t) = log(\\frac{1+N}{1+n_t})$$  \n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;where:  \n",
    "$$\\begin{align}f_{t,d} &= \\text{count of term } \\textbf{t} \\text{ in document } \\textbf{d} \\\\  \n",
    "N &= \\text{total number of documents} \\\\  \n",
    "n_t &= \\text{number of documents that contain term } \\textbf{t}\\end{align}$$  \n",
    "  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;We added 1 in the first logarithm to avoid getting $-\\infty$ when $f_{t,d}$ is 0. In the second logarithm we added one fake document to avoid division by zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kt7bdF819lCp"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Before we transform our data into vectors of counts or tf-idf values we should remove English **stopwords**<sup>(6)(7)</sup>. Stopwords are words that are very common in a language and are usually removed in the preprocessing stage of natural text-related tasks like sentiment analysis or search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p5_MOC6Z9lCp"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Note that we should construct our vocabulary only based on the training set. When we will process the test data in order to make predictions we should use only the vocabulary constructed in the training phase, the rest of the words will be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DicmPJO09lCq"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Now, let's create the data frames from the supplied csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hw_fmTEX9lCq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GE1zgTOi9lCs"
   },
   "outputs": [],
   "source": [
    "# Read in the training and test datasets from previously created csv files\n",
    "\n",
    "imdb_train = pd.read_csv('csv/imdb_train.csv')\n",
    "imdb_test = pd.read_csv('csv/imdb_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yPYggSnd9lCu",
    "outputId": "580e2d8e-c864-413b-a2ce-6b5737254b6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Training dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    25000 non-null  object\n",
      " 1   label   25000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 390.8+ KB\n",
      "Training dataset Content:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    text  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                          This is what I was expecting when star trek DS9 premiered. Not to slight DS9. That was a wonderful show in it's own right, however it never really gave the fans more of what they wanted. Enterprise is that show. While having a similarity to the original trek it differs enough to be original in it's own ways. It makes the ideas of exploration exciting to us again. And that was one of the primary ingredients that made the original so loved. Another ingredient to success was the relationships that evolved between the crew members. Viewers really cared deeply for the crew. Enterprise has much promise in this area as well. The chemistry between Bakula and Blalock seems very promising. While sexual tension in a show can often become a crutch, I feel the tensions on enterprise can lead to much more and say alot more than is typical. I think when we deal with such grand scale characters of different races or species even, we get some very interesting ideas and television. Also, we should note the performances, Blalock is very convincing as Vulcan T'pol and Bacula really has a whimsy and strength of character that delivers a great performance. The rest of the cast delivered good performances also. My only gripes are as follows. The theme. It's good it's different, but a little to light hearted for my liking. We need something a little more grand. Doesn't have to be orchestral. Maybe something with a little more electronic sound would suffice. And my one other complaint. They sell too many adds. They could fix this by selling less ads, or making all shows two parters. Otherwise we'll end up seeing the shows final act getting wrapped up way too quickly as was one of my complaints of Voyager.   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          This film reminded me so much of \"A History of Violence\" which pretended to be a close study of violence and violent behavior but ended up just being nothing short of a cheap action movie masquerading as some thinking film on violence. Dustin Hoffman and his new British bride move to a small English town and encounter endless harassment from the local drunks who do nothing but hang at the pub all day and make trouble. Don't these men have a job? Anyway, Dustin takes all he can take and by the end of the film he holds up in his house and fights off each one of the drunk attackers by such gruesome means as boiling whiskey poured over someone, feet being blown off by a shotgun and someones head getting caught in a bear trap. Funny that someone would have a need for such a large bear trap in a small British town except maybe put a mans head in it. Sam Peckinpah who made the \"Wild Bunch\" which also covered the topic of blood letting violence in which no one was spared. But it was done with style, and you believed it. Straw Dogs is not believable. First of all the location is wrong and does not work. Why place it in England? I would think maybe in some inner city location or a small town in the American South in the 1930's or something. Second it is not in my view ever really explained clearly why these men are so quick to violence except maybe they got drunk and felt a need to kill Hoffman and rape his wife. Sam Peckinpah missed the mark on this one.   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Theodore Rex is possibly the greatest cinematic experience of all time, and is certainly a milestone in human culture, civilization and artistic expression!! In this compelling intellectual masterpiece, Jonathan R. Betuel aligns himself with the great film makers of the 20th century, such as Francis Ford Copola, Martin Scorcese, Orson Welles and Roman Polanski. The special effects are nothing less than breathtaking, and make any work by Spielberg look trite and elementary. At the time of it's release, Theodore Rex was such a revolutionary gem that it raised the bar of film-making to levels never anticipated by film makers. The concept of making not just a motion picture featuring a dinosaur, but adapting an action packed, thrilling detective novel, co-staring a \"talking\" dinosaur with a post-modern name such as \"Theodore\", and an existential female police officer changed humanity as we know it. The world could never be the same after experiencing such magnificent beauty. Watching Theodore Rex is much akin to looking into the face of God and hearing Him say \"you are my most beloved creation.\" This is one of the few films that is simply TO DIE FOR!!!   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Although I didn't like Stanley & Iris tremendously as a film, I did admire the acting. Jane Fonda and Robert De Niro are great in this movie. I haven't always been a fan of Fonda's work but here she is delicate and strong at the same time. De Niro has the ability to make every role he portrays into acting gold. He gives a great performance in this film and there is a great scene where he has to take his father to a home for elderly people because he can't care for him anymore that will break your heart. I wouldn't really recommend this film as a great cinematic entertainment, but I will say you won't see much bette acting anywhere.   \n",
      "4  'De Grot' is a terrific Dutch thriller, based on the book written by Tim Krabbé. Another of his books, 'Het Gouden Ei' was made into the great Dutch mystery thriller called 'Spoorloos' ('The Vanishing') in 1988. This one is not as good as that thriller (although much better than the American remake also called 'The Vanishing') but there are times it comes close. Especially the opening moments are terrific. We see a man, later we learn his name is Egon Wagter (Fedja van Huêt), coming from a plane in Thailand. When he picks up his bags it is pretty clear that he is smuggling something across the border. These scenes are perfectly directed, photographed and acted. A kind of suspense is created that you would normally not have in an opening scene like this. Later we see how Egon makes his deal in Thailand with a woman, both stating that they have never done anything like this. From this point the movie is constantly flashback and flash-forward. We see how Egon, still as a child (here played by Erik van der Horst), befriends a guy named Axel (as a kid played by Benja Bruijning). We learn how they grew up as friends, sort of, and how Axel (as an adult played by Marcel Hensema) became a criminal. Egon in the meanwhile goes to college and settles with a woman. Around this time he sometimes meets Axel but does not really want anything to do with him. The movie is chronological in a way. It shows Egon and Axel as kids, than as students, young adults, and in their mid-thirties. But from time to time, like I said, the movie goes back to when they were kids and jumps forward again. Every time we see them as kids it explains something that happens when they are adults. Minor spoilers herein. The title means 'The Cave', and it is the cave that gives the movie its happy ending, although it is in fact not that happy. Like the beginning, the ending is terrific. The middle part of the movie is entertaining and in a way it distracts our attention of the first scenes, only to come back at that point in the end. It is the editing that gives the movie its happy ending, although we can say the dramatic ending is happy in a way as well.   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "1      0  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n",
      "\n",
      "----- Test dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    25000 non-null  object\n",
      " 1   label   25000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 390.8+ KB\n",
      "Test dataset Content:\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     text  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Progeny is about a husband and wife who experience time loss while making love. Completely unaware of what this bizarre experience means they try to go on with their lives. The hubby begins questioning the bizarre event and gets help through a very annoying psychiatrist. He comes to believe that aliens are responsible for this lapse in time and that the unborn baby he once thought was his and his wife's actually belongs to the aliens. If ya ask me, this is a great scifi/horror story. Taking a highly questionable real-life scenario involving alien abduction and hybrid breeding is definite thumbs up from this guy. I love all things related to aliens and this story definitely delivered some good ideas. So if you also share an interest in things extraterrestrial, you should be pretty happy with Progeny. At least story-wise anyways. Unfortunately the movie overall is pretty average. With average acting by all actors. Yep, even by the consistently awesome Mr. Dourif, who still does deliver the best performance. Though the black head doctor, delivers his lines really well. There are a few points in the flick where some of the delivery is cringe or laugh worthy, which is fine in my book. I like them cheesy and this had a little bit of some nice stinky cheese, and I mean that in a good way. Anyways, with a less than stellar script you can't really blame all the actors. I especially didn't care for the Mother Hysteria the film went for. She wanted a baby so badly that she'd neglect and dismiss everything her loving husband (who's a doctor!!) said to her. It almost reached a point where you actually didn't care what happened to her. The Progeny is another flick by Brian Yuzna from the icky-sticky film, Society. Again he delivers some slimy effects, and again he delivers a pretty unique tale of horror. If you're into scifi/horror or are a fan of Dourif and or Yuzna films, there's no real reason not to check out this flick if you get the chance. A generous 7 outta 10.   \n",
      "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         This movie is very similar to Death Warrant with Jean-Claude Van Damme and also has some similarities to Island of Fire with Jackie Chan and I also heard that there is some other very similar action movies, but this film has a much better action than Death Warrant or even Island of Fire (that's right, the Jackie Chan's movie). Rarely American action movies has such a great action sequences, though there was many negative reviews on this film, it easily beats most of the action movies of that time who were more successful. There were many martial art's scenes, David Bradley was fast as Bruce Lee in this film and what else was good, that fighting scenes were much longer than in most of the American martial art's movies. The shoot-out scenes were similar with John Woo's movies, maybe not that good, but still very exiting. There was also many impressive explosions and one great chase scene. I've seen some other David Bradley's movies, but this one, yet is the best in terms of action. OK, this movie has some cheesy moments, but which movie hasn't? The acting was decent, Charles Napier was incredible and his character was real tough. Adam Clark who played Squid and Yuji Okumoto who played the main bad guy were also very good. Other actors acted pretty well too, though the acting isn't important in this type of movies. If you are action movies fan (I mean the real action movies fan, who really can appreciate the good action), than you must see this film.   \n",
      "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       This is an excellent movie and I would recommend it to everyone. Mr. Drury's acting is top notch as it always is and he blends well with the other actors in the movie. Can't give away any of the suspense or drama found in the movie. Hell to pay is a must see movie!!! The plot was very suspenseful. I would watch this movie over and over again because it has all the elements of a great western movie. It was very authentic in how they displayed the components dealing with this movie which includes the guns, horses, and clothing. The soundtrack is enjoyable and adds flavor to the movie. James Drury has the right touch when picking out a movie to be involved with. This is a another winner for the western genre. !!!!!   \n",
      "3  The 13th and last RKO Falcon film starts with the mutual injunction by Tom Conway as Tom Lawrence alias the Falcon and Ed Brophy as Goldie of \"No dames!\" whilst they prepare to go on vacation. While you're still wondering what they're going on vacation from as they hadn't had a job since the beginning of the 1st film in 1941 (with Sanders as Gay though and Jenkins as Goldie) they bump into a woman and get dragged into a seedy industrial espionage caper. They promise to help her when her uncle is murdered, by taking an envelope containing the details of a formula to make substitute industrial diamonds to his business colleague in Miami. Suspect everyone here except the cops here who are after Lawrence  and Goldie for the murder. To console himself Goldie keeps paraphrasing travel brochures: \"On the coldest day you can always enjoy the warmth of a nice cosy electric chair\" for one. Some nice languid atmospheric nightclub scenes rub shoulders with some especially bad behaviour from the baddies. Favourite bit: the dignified game of hide and seek/hunt the thimble the imperturbable and suave Lawrence has with the baddies on the sleeper train. Least favourite bit: the most embarrassing scene in the entire series in the alligator wrestling hut  definitely thrown in for the kids! All in all not the best in the series but yet another entertaining outing, with an overall satisfying plot and many episodes even in this that make me wish they could have gone on for just a few more years as Columbia did with Boston Blackie, although RKO were churning these out faster. Absolutely no sex, not much violence (in fact none at all by today's high standards), and positively no message all make this type of film anathema to serious people who can only regard movies as an art form that must depend on these three pillars. Three Diet Falcon's were made later with John Calvert in the title role, I don't mind them but could never bring myself to count them into the main series, which Tom Conway had made his own by this time. Sad also that it was all downhill after this for Conway, who moved into TV, voice overs and even played Norman Conquest in Park Plaza 605 rather well in 1953. He also developed serious eye and alcohol problems  I don't know if they were linked  wound up poverty stricken and after a spell in hospital in 1967 was found dead in his girlfriend's bed. For us folk that want to at least we still have his 10 entertaining Falcon's plus a number of other worthy, even classic RKO movies from 1942 to 1946 with which to remember him by.   \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        I LOVE Dr WHo SO much! I believe that David Tennant is the best Dr the show has ever had and Billie Piper the Best companion! I liked the way the Dr and Rose had such a connection and a great relationship and the Dr came close a few times to expressing his love for rose! It sadly came to an end after only 2 seasons. I will miss watching rose heaps and think that the show will not be the same without Rose! But David is still there to make me laugh and make me happy to watch him play this fantastic role! I rate this show 110% it is FANTASTIC! The graphics and monsters in this show are wonderful and every storyline is different but somewhat connected and i have actually learned somethings about love, the world and relationships from this show. Therefore it must be one of the most fantastic shows of all time!   \n",
      "\n",
      "   label  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n"
     ]
    }
   ],
   "source": [
    "# Display information and first few entries from the training and test datasets\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print (\"----- Training dataset Info:\")\n",
    "imdb_train.info(verbose=None, buf=None, max_cols=None, memory_usage=None, null_counts=None)\n",
    "print (\"Training dataset Content:\")\n",
    "print(imdb_train.iloc[:5])\n",
    "\n",
    "print (\"\\n----- Test dataset Info:\")\n",
    "imdb_test.info(verbose=None, buf=None, max_cols=None, memory_usage=None, null_counts=None)\n",
    "print (\"Test dataset Content:\")\n",
    "print(imdb_test.iloc[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-COMQ8dY9lCw"
   },
   "source": [
    "### Text vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AtrcxAUp9lCx"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Fortunately, for the text vectorization part all the hard work is already done in the Scikit-Learn classes `CountVectorizer`<sup>(8)</sup> and `TfidfTransformer`<sup>(5)</sup>. We will use these classes to transform our csv files into unigram and bigram matrices(using both counts and tf-idf values). (It turns out that if we only use a n-gram for a large n we don't get a good accuracy, we usually use all n-grams up to some n. So, when we say here bigrams we actually refer to uni+bigrams and when we say unigrams it's just unigrams.) Each row in those matrices will represent a document (review) in our dataset, and each column will represent values associated with each word in the vocabulary (in the case of unigrams) or values associated with each combination of maximum 2 words in the vocabulary (bigrams).  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;`CountVectorizer` has a parameter `ngram_range` which expects a tuple of size 2 that controls what n-grams to include. After we constructed a `CountVectorizer` object we should call `.fit()` method with the actual text as a parameter, in order for it to learn the required statistics of our collection of documents. Then, by calling `.transform()` method with our collection of documents it returns the matrix for the n-gram range specified. As the class name suggests, this matrix will contain just the counts. To obtain the tf-idf values, the class `TfidfTransformer` should be used. It has the `.fit()` and `.transform()` methods that are used in a similar way with those of `CountVectorizer`, but they take as input the counts matrix obtained in the previous step and `.transform()` will return a matrix with tf-idf values. We should use `.fit()` only on training data and then store these objects. When we want to evaluate the test score or whenever we want to make a prediction we should use these objects to transform the data before feeding it into our classifier.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Note that the matrices generated for our train or test data will be huge, and if we store them as normal numpy arrays they will not even fit into RAM. But most of the entries in these matrices will be zero. So, these Scikit-Learn classes are using Scipy sparse matrices<sup>(9)</sup> (`csr_matrix`<sup>(10)</sup> to be more exactly), which store just the non-zero entries and save a LOT of space.  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;We will use a linear classifier with stochastic gradient descent, `sklearn.linear_model.SGDClassifier`<sup>(11)</sup>, as our model. First we will generate and save our data in 4 forms: unigram and bigram matrix (with both counts and tf-idf values for each). Then we will train and evaluate our model for each these 4 data representations using `SGDClassifier` with the default parameters. After that, we choose the data representation which led to the best score and we will tune the hyper-parameters of our model with this data form using cross-validation in order to obtain the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xmfPejcN9lCx"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s6b7Oqp-9lCz"
   },
   "source": [
    "#### Unigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dArWvRR9lCz",
    "outputId": "304f63a1-3c5a-48ec-f708-8236e0003862"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a unigram vectorizer and process the training set to generate a list of words. \n",
    "# Note that unigram processing is set via the ngram_range parameter\n",
    "\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "unigram_vectorizer.fit(imdb_train['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHGOebuq9lC2",
    "outputId": "86a3ee95-00e0-4b5d-d0c3-5bfc765e9725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words found: 74849\n",
      "['bête', 'bûsu', 'bürgermeister', 'c1', 'c3', 'c3p0', 'c3po', 'c4', 'c57', 'c__p', 'ca', 'caaaaaaaaaaaaaaaaaaaaaaligulaaaaaaaaaaaaaaaaaaaaaaa', 'caalling', 'caan', 'caas', 'cab', 'cabal', 'caballe', 'caballeros', 'caballo', 'cabals', 'cabana', 'cabanne', 'cabaret', 'cabarnet', 'cabbage', 'cabbages', 'cabbie', 'cabby', 'cabel', 'cabell', 'cabells', 'cabin', 'cabinet', 'cabinets', 'cabins', 'cabiria', 'cable', 'cables', 'cabo', 'cabot', 'cabrón', 'cabs', 'caca', 'caccia', 'cache', 'cachet', 'cacho', 'cack', 'cackle', 'cackles', 'cackling', 'caco', 'cacophonist', 'cacophonous', 'cacophony', 'cacoyanis', 'cacoyannis', 'cacti', 'cactus', 'cactuses', 'cad', 'cada', 'cadaver', 'cadaverous', 'cadavers', 'cadavra', 'cadby', 'caddie', 'caddy', 'caddyshack', 'cadena', 'cadence', 'cadences', 'cadet', 'cadets', 'cadfile', 'cadilac', 'cadillac', 'cadillacs', 'cadmus', 'cadre', 'cads', 'caduta', 'cady', 'caesar', 'caesars', 'caeser', 'caetano', 'cafe', 'cafes', 'cafeteria', 'caffari', 'caffeinated', 'caffeine', 'cafferty', 'caffey', 'café', 'cafés', 'cage']\n",
      "['pincher', 'pinchers', 'pinches', 'pinching', 'pinchot', 'pinciotti', 'pine', 'pineal', 'pineapple', 'pineapples', 'pines', 'pinet', 'pinetrees', 'pineyro', 'pinfall', 'pinfold', 'ping', 'pingo', 'pinhead', 'pinheads', 'pinho', 'pining', 'pinjar', 'pink', 'pinkerton', 'pinkett', 'pinkie', 'pinkins', 'pinkish', 'pinko', 'pinks', 'pinku', 'pinkus', 'pinky', 'pinnacle', 'pinnacles', 'pinned', 'pinning', 'pinnings', 'pinnochio', 'pinnocioesque', 'pino', 'pinocchio', 'pinochet', 'pinochets', 'pinoy', 'pinpoint', 'pinpoints', 'pins', 'pinsent', 'pint', 'pinta', 'pinter', 'pintilie', 'pinto', 'pintos', 'pints', 'pinup', 'pioneer', 'pioneered', 'pioneering', 'pioneers', 'piotr', 'pious', 'piovani', 'pip', 'pipe', 'piped', 'pipedream', 'pipeline', 'piper', 'pipers', 'pipes', 'piping', 'pippi', 'pippin', 'pipsqueak', 'piquant', 'piquantly', 'pique', 'piqued', 'piquer', 'piquor', 'piracy', 'pirahna', 'piranha', 'piranhas', 'pirate', 'pirated', 'piraters', 'pirates', 'pirotess', 'pirouette', 'pirouettes', 'pirouetting', 'pirovitch', 'pirro', 'pis', 'pisa', 'pisana']\n"
     ]
    }
   ],
   "source": [
    "# Display the length and a few samples of the unigram vectorizer to show the words that have been extracted\n",
    "\n",
    "print(\"Number of words found:\", len(unigram_vectorizer.get_feature_names()))\n",
    "print(unigram_vectorizer.get_feature_names()[10000:10100])\n",
    "print(unigram_vectorizer.get_feature_names()[50000:50100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xgehvpkr9lC3"
   },
   "outputs": [],
   "source": [
    "# Now process the training dataset to get a count of the words extracted earlier\n",
    "\n",
    "X_train_unigram = unigram_vectorizer.transform(imdb_train['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eqEqnSZZ9lC5",
    "outputId": "b3d9065b-f02b-4ed4-e20f-f465e11ab9db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# Display the attributes the word count matrix; notice it is huge with 25000 rows since we have 25000 entries\n",
    "# in the training dataset and 74849 columns since we saw above that we have a vocabulary of 74849 words\n",
    "\n",
    "print(repr(X_train_unigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwUZxydJ9lC7"
   },
   "source": [
    "#### Unigram Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xx0FsrgL9lC7",
    "outputId": "d13a68ed-8292-4a9b-faa9-8265b85a7b0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a unigram tf-idf vectorizer and load the training set using the word count matrix from earlier\n",
    "\n",
    "unigram_tf_idf_transformer = TfidfTransformer()\n",
    "unigram_tf_idf_transformer.fit(X_train_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pREGYWEg9lC9"
   },
   "outputs": [],
   "source": [
    "# Now calculate the unigram tf-idf statistics\n",
    "\n",
    "X_train_unigram_tf_idf = unigram_tf_idf_transformer.transform(X_train_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfUMU4DW9lC_",
    "outputId": "21653fb8-4354-4322-e4c8-adc9ed39e636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x74849 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# Display the attributes the unigram tf-idf matrix; it should be the same size as the unigram matrix above\n",
    "\n",
    "print(repr(X_train_unigram_tf_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9GIwBJKM9lDB"
   },
   "source": [
    "#### Bigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l00sYvzX9lDB",
    "outputId": "52748c55-184b-409b-c8ec-4c2728e93b79"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bigram vectorizer and process the training set to generate a list of bigrams. \n",
    "# Note that bigram processing is set via the ngram_range parameter and so includes unigrams and bigrams\n",
    "\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "bigram_vectorizer.fit(imdb_train['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zlRED15i9lDD",
    "outputId": "c0fba0ba-d30c-4308-a8c2-dfa0c3527d53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bigrams found: 1520266\n",
      "['3am but', '3am invesment', '3am it', '3am taped', '3bs', '3bs who', '3d', '3d adventure', '3d although', '3d and', '3d animated', '3d animation', '3d animations', '3d animators', '3d artists', '3d assante', '3d bird', '3d bore', '3d capabilities', '3d cg', '3d cgi', '3d character', '3d companies', '3d computer', '3d dept', '3d disgrace', '3d effect', '3d effects', '3d element', '3d ending', '3d environment', '3d especially', '3d feel', '3d game', '3d games', '3d glasses', '3d grafics', '3d graphics', '3d gravity', '3d had', '3d imax', '3d in', '3d it', '3d just', '3d mario', '3d models', '3d movie', '3d movies', '3d panoramic', '3d plat', '3d probably', '3d programs', '3d sequence', '3d shoot', '3d shooter', '3d shooters', '3d special', '3d splatter', '3d technology', '3d that', '3d trust', '3d was', '3d wave', '3d well', '3d which', '3d with', '3d world', '3d wow', '3dvd', '3dvd collection', '3k', '3k all', '3k attack', '3k but', '3k if', '3k it', '3k not', '3lbs', '3lbs is', '3m', '3m to', '3mins', '3mins jack', '3p', '3p felt', '3p uses', '3p wasn', '3p0', '3p0 r2', '3pm', '3pm and', '3po', '3po and', '3po frank', '3po mind', '3rd', '3rd 4th', '3rd act', '3rd and', '3rd annual']\n",
      "['allen radiation', 'allen randall', 'allen really', 'allen reprises', 'allen reprising', 'allen retains', 'allen retired', 'allen rivkin', 'allen screen', 'allen scripts', 'allen second', 'allen seems', 'allen sees', 'allen set', 'allen she', 'allen should', 'allen showed', 'allen shows', 'allen since', 'allen slowly', 'allen small', 'allen smithee', 'allen soule', 'allen starrer', 'allen stereotypical', 'allen still', 'allen straight', 'allen stuff', 'allen stuttering', 'allen style', 'allen such', 'allen surprisingly', 'allen surrogate', 'allen take', 'allen technique', 'allen tells', 'allen the', 'allen they', 'allen think', 'allen this', 'allen ticks', 'allen to', 'allen treat', 'allen waking', 'allen wannabe', 'allen warning', 'allen was', 'allen we', 'allen when', 'allen whining', 'allen who', 'allen will', 'allen williams', 'allen with', 'allen work', 'allen would', 'allen writes', 'allen writing', 'allen yes', 'allen you', 'allen young', 'allende', 'allende and', 'allende author', 'allende book', 'allende characters', 'allende has', 'allende killed', 'allende magical', 'allende not', 'allende novel', 'allende overthrown', 'allende should', 'allende was', 'allende who', 'allens', 'allens stuff', 'allergic', 'allergic are', 'allergic reaction', 'allergic reactions', 'allergic to', 'allergies', 'allergies kept', 'allergy', 'allergy senator', 'allergy to', 'alles', 'alles action', 'alleviate', 'alleviate her', 'alleviate pain', 'alleviate the', 'alleviate their', 'alleviate visual', 'alley', 'alley 1912', 'alley about', 'alley acting', 'alley again']\n"
     ]
    }
   ],
   "source": [
    "# Display the length and a few samples of the bigram vectorizer to show the bigrams that have been extracted\n",
    "\n",
    "print(\"Number of bigrams found:\", len(bigram_vectorizer.get_feature_names()))\n",
    "print(bigram_vectorizer.get_feature_names()[10000:10100])\n",
    "print(bigram_vectorizer.get_feature_names()[50000:50100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W2iPN3dv9lDF"
   },
   "outputs": [],
   "source": [
    "# Now generate bigram statistics on the training set\n",
    "\n",
    "X_train_bigram = bigram_vectorizer.transform(imdb_train['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dxQaYkVg9lDG",
    "outputId": "03cd3b38-0b4d-4e17-fcd2-cff8b2b3db89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x1520266 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 8689547 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# Display the attributes the bigram count matrix; notice it is really huge with 25000 rows since we have 25000 entries\n",
    "# in the training dataset and 1520266 columns since we saw above that we have 1520266 bigrams\n",
    "\n",
    "print(repr(X_train_bigram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qb745OZ79lDJ"
   },
   "source": [
    "#### Bigram Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y_o-dnsd9lDJ",
    "outputId": "5e7a69bc-6cdf-479b-abfe-2a669d57be10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bigram tf-idf vectorizer and load the training set using the bigram count matrix from earlier\n",
    "\n",
    "bigram_tf_idf_transformer = TfidfTransformer()\n",
    "bigram_tf_idf_transformer.fit(X_train_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "knxsmxUu9lDL"
   },
   "outputs": [],
   "source": [
    "# Now calculate the bigram tf-idf statistics\n",
    "\n",
    "X_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HOVm5loL9lDN",
    "outputId": "2068dd54-f136-450a-8820-6da40a83142d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x1520266 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 8689547 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# Display the attributes the bigram tf-idf matrix; it should be the same size as the bigram matrix above\n",
    "\n",
    "print(repr(X_train_bigram_tf_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a trigram vectorizer and process the training set to generate a list of words. \n",
    "# Note that unigram processing is set via the ngram_range parameter\n",
    "\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "trigram_vectorizer.fit(imdb_train['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words found: 74849\n",
      "['1938 when', '1938 when happen', '1938 when hitler', '1938 with', '1938 with frederic', '1938 year', '1938 year in', '1939', '1939 13', '1939 13 tough', '1939 1942', '1939 1942 desert', '1939 1944', '1939 1944 via', '1939 40', '1939 40 and', '1939 45', '1939 45 there', '1939 and', '1939 and as', '1939 and compare', '1939 and gone', '1939 and he', '1939 and tells', '1939 and that', '1939 and the', '1939 and then', '1939 as', '1939 as seen', '1939 as someone', '1939 behind', '1939 behind the', '1939 buchanan', '1939 buchanan holds', '1939 but', '1939 but probably', '1939 but whose', '1939 by', '1939 by the', '1939 charlie', '1939 charlie who', '1939 despite', '1939 despite the', '1939 distant', '1939 distant war', '1939 douglas', '1939 douglas plays', '1939 england', '1939 england as', '1939 eugene', '1939 eugene palette', '1939 even', '1939 even more', '1939 filled', '1939 filled with', '1939 film', '1939 film from', '1939 film gunga', '1939 film tried', '1939 first', '1939 first and', '1939 fringe', '1939 fringe benefit', '1939 gwtw', '1939 gwtw he', '1939 highly', '1939 highly recommended', '1939 hitler', '1939 hitler took', '1939 in', '1939 in that', '1939 is', '1939 is based', '1939 is lackadaisical', '1939 is universally', '1939 it', '1939 it has', '1939 it will', '1939 its', '1939 its different', '1939 leaves', '1939 leaves about', '1939 may', '1939 may 1940', '1939 military', '1939 military swashbuckler', '1939 moved', '1939 moved to', '1939 mystery', '1939 mystery comedy', '1939 needless', '1939 needless to', '1939 now', '1939 now the', '1939 one', '1939 one of', '1939 out', '1939 out of', '1939 release', '1939 release but']\n",
      "['about vampire', 'about vampire bats', 'about vampire vs', 'about vampire wars', 'about vampires', 'about vampires aliens', 'about vampires and', 'about vampires but', 'about vampires of', 'about vampires returning', 'about vampires she', 'about vampires that', 'about vampires vampires', 'about vampires with', 'about vansishing', 'about vansishing point', 'about variety', 'about variety of', 'about various', 'about various 1994', 'about various characters', 'about various obvious', 'about various people', 'about various sex', 'about various theories', 'about vast', 'about vast array', 'about vd', 'about vd scrotum', 'about ve', 'about ve feeling', 'about ve never', 'about venezuela', 'about venezuela it', 'about venezuela or', 'about venezuelan', 'about venezuelan civilian', 'about venezuelan politics', 'about vengeance', 'about vengeance for', 'about verhoeven', 'about verhoeven film', 'about very', 'about very awkward', 'about very briefly', 'about very charismatic', 'about very clever', 'about very important', 'about very little', 'about very random', 'about very real', 'about very sad', 'about very sexy', 'about very unusual', 'about very very', 'about very vigilant', 'about vibrant', 'about vibrant community', 'about vibrant young', 'about vic', 'about vic who', 'about vick', 'about vick world', 'about vicky', 'about vicky and', 'about victorian', 'about victorian era', 'about viennese', 'about viennese people', 'about vietnam', 'about vietnam and', 'about vietnam or', 'about vietnam vet', 'about vietnam vets', 'about viewers', 'about viewers that', 'about viewing', 'about viewing her', 'about viewings', 'about viewings over', 'about views', 'about views of', 'about vigilantism', 'about vigilantism in', 'about viking', 'about viking has', 'about viking warlord', 'about village', 'about village that', 'about villages', 'about villages with', 'about violence', 'about violence in', 'about violent', 'about violent backlash', 'about violent crimes', 'about violent game', 'about violent mayhem', 'about virginia', 'about virginia anderson']\n"
     ]
    }
   ],
   "source": [
    "# Display the length and a few samples of the trigram vectorizer to show the words that have been extracted\n",
    "\n",
    "print(\"Number of words found:\", len(unigram_vectorizer.get_feature_names()))\n",
    "print(trigram_vectorizer.get_feature_names()[10000:10100])\n",
    "print(trigram_vectorizer.get_feature_names()[50000:50100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now process the training dataset to get a count of the words extracted earlier\n",
    "\n",
    "X_train_trigram = trigram_vectorizer.transform(imdb_train['text'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tf_idf_transformer = TfidfTransformer()\n",
    "trigram_tf_idf_transformer.fit(X_train_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now calculate the trigram tf-idf statistics\n",
    "\n",
    "X_train_trigram_tf_idf = trigram_tf_idf_transformer.transform(X_train_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<25000x5181124 sparse matrix of type '<class 'numpy.float64'>'\n",
      "\twith 14166465 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "# Display the attributes the trigram tf-idf matrix; it should be the same size as the trigram matrix above\n",
    "\n",
    "print(repr(X_train_trigram_tf_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YjConCNN9lDO"
   },
   "source": [
    "### Try the four different data formats (unigram, bigram with and without tf_idf) on the training set and pick the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a2Eno3PI9lDO"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;Now, for each data form we split it into train & validation sets, train a `SGDClassifier` and output the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OL738RU39lDP"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-df07f438c93b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OOLANzU9lDQ"
   },
   "outputs": [],
   "source": [
    "# Helper function to display confusion matrix\n",
    "\n",
    "def display_confusion_matrix(y_true, y_pred) -> None:\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in cf_matrix.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='coolwarm')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCMyDFTa9lDS"
   },
   "outputs": [],
   "source": [
    "def train_and_show_scores(X: csr_matrix, y: np.array, title: str) -> Tuple[float, float]:\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X, y, train_size=0.75, stratify=y\n",
    "    )\n",
    "\n",
    "    clf = SGDClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_score = clf.score(X_train, y_train)\n",
    "    valid_score = clf.score(X_valid, y_valid)\n",
    "    print(f'{title}\\nTrain score: {round(train_score, 2)} ; Validation score: {round(valid_score, 2)}')\n",
    "    \n",
    "    train_pred = clf.predict(X_train)\n",
    "    valid_pred = clf.predict(X_valid)\n",
    "    print(f'Train precision: {round(precision_score(y_train, train_pred), 2)} ; Validation precision: {round(precision_score(y_valid, valid_pred), 2)}')\n",
    "    print(f'Train recall: {round(recall_score(y_train, train_pred), 2)} ; Validation recall: {round(recall_score(y_valid, valid_pred), 2)}')\n",
    "    print(f'Train F1: {round(f1_score(y_train, train_pred), 2)} ; Validation F1: {round(f1_score(y_valid, valid_pred), 2)}')\n",
    "    print(\"Train Confusion Matrix: \")\n",
    "    print(confusion_matrix(y_train, train_pred))\n",
    "    display_confusion_matrix(y_train, train_pred)\n",
    "    print(\"Validation Confusion Matrix: \")\n",
    "    print(confusion_matrix(y_valid, valid_pred))\n",
    "    display_confusion_matrix(y_valid, valid_pred)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return train_score, valid_score, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oz2vhlp49lDU"
   },
   "outputs": [],
   "source": [
    "y_train = imdb_train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jAfpF9iX9lDW",
    "outputId": "f51dcfa9-6d2f-46d1-df5e-a92689d240fc"
   },
   "outputs": [],
   "source": [
    "uc_train_score, uc_valid_score, uc_clf = train_and_show_scores(X_train_unigram, y_train, '----- Unigram Counts -----')\n",
    "utfidf_train_score, utfidf_valid_score, utfidf_clf = train_and_show_scores(X_train_unigram_tf_idf, y_train, '----- Unigram Tf-Idf -----')\n",
    "bc_train_score, bc_valid_score, bc_clf = train_and_show_scores(X_train_bigram, y_train, '----- Bigram Counts -----')\n",
    "btfidf_train_score, btfidf_valid_score, btfidf_clf = train_and_show_scores(X_train_bigram_tf_idf, y_train, '----- Bigram Tf-Idf -----')\n",
    "tc_train_score, tc_valid_score, tc_clf = train_and_show_scores(X_train_trigram, y_train, '----- Trigram Counts -----')\n",
    "ttfidf_train_score, ttfidf_valid_score, ttfidf_clf = train_and_show_scores(X_train_trigram_tf_idf, y_train, '----- Trigram Tf-Idf -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "McGxPaXb9lDX",
    "outputId": "8ac7fdcf-dcdc-467a-dcdb-fbf3ba1d3da2"
   },
   "outputs": [],
   "source": [
    "# Display the previously derived scores for the four scenarios\n",
    "\n",
    "sns.set_style(\"whitegrid\", {'grid.linestyle': '--'})\n",
    "\n",
    "print (\"Training score for the four approaches:\")\n",
    "ax1 = sns.barplot(\n",
    "    x= ['Unigram Count', 'Unigram tf-idf', 'Bigram Count', 'Bigram tf-idf'],\n",
    "    y= [uc_train_score, utfidf_train_score, bc_train_score, btfidf_train_score])\n",
    "ax1.set(ylim=(0.8, 1.0))\n",
    "plt.show()\n",
    "\n",
    "print (\"Validation score for the four approaches:\")\n",
    "ax2 = sns.barplot(\n",
    "    x= ['Unigram Count', 'Unigram tf-idf', 'Bigram Count', 'Bigram tf-idf'],\n",
    "    y= [uc_valid_score, utfidf_valid_score, bc_valid_score, btfidf_valid_score])\n",
    "ax2.set(ylim=(0.8, 1.0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE0LbIvB9lDa"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;The best data form seems to be **bigram with tf-idf** as it gets the highest validation accuracy: **0.9**; so we will choose it as our preferred approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gD4vT4Uc9lDb"
   },
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DFFtigA9lDb"
   },
   "outputs": [],
   "source": [
    "# Transform the test data set into the bigram tf-idf format\n",
    "\n",
    "X_test = bigram_vectorizer.transform(imdb_test['text'].values)\n",
    "X_test = bigram_tf_idf_transformer.transform(X_test)\n",
    "y_test = imdb_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJYdvnUH9lDc",
    "outputId": "1f2143cf-4c04-483a-c115-8d1ef4f17b18"
   },
   "outputs": [],
   "source": [
    "# Now evaluate the test data using the previously trained bigram tf-idf classifier\n",
    "\n",
    "clf = btfidf_clf\n",
    "score = clf.score(X_test, y_test)\n",
    "\n",
    "print(f'Score: {round(score, 4)}')\n",
    "\n",
    "test_pred = clf.predict(X_test)\n",
    "print(f'Test precision: {round(precision_score(y_test, test_pred), 4)}')\n",
    "print(f'Test recall: {round(recall_score(y_test, test_pred), 4)}')\n",
    "print(f'Test F1: {round(f1_score(y_test, test_pred), 4)}')\n",
    "print(\"Test Confusion Matrix: \")\n",
    "print(confusion_matrix(y_test, test_pred))\n",
    "display_confusion_matrix(y_test, test_pred)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AeOTS2Qo9lDe"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;And we got almost 90% test accuracy. That's not bad for our simple linear model. There are more advanced methods that give better results. The current state-of-the-art on this dataset is **97.42%** <sup>(13)</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_rNamQ09lDe"
   },
   "source": [
    "## References\n",
    "\n",
    "<sup>(1)</sup> &nbsp;[Sentiment Analysis - Wikipedia](https://en.wikipedia.org/wiki/Sentiment_analysis)  \n",
    "<sup>(2)</sup> &nbsp;[Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)  \n",
    "<sup>(3)</sup> &nbsp;[Bag-of-words model - Wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)  \n",
    "<sup>(4)</sup> &nbsp;[Tf-idf - Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)  \n",
    "<sup>(5)</sup> &nbsp;[TfidfTransformer - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)  \n",
    "<sup>(6)</sup> &nbsp;[Stop words - Wikipedia](https://en.wikipedia.org/wiki/Stop_words)  \n",
    "<sup>(7)</sup> &nbsp;[A list of English stopwords](https://gist.github.com/sebleier/554280)  \n",
    "<sup>(8)</sup> &nbsp;[CountVectorizer - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)  \n",
    "<sup>(9)</sup> &nbsp;[Scipy sparse matrices](https://docs.scipy.org/doc/scipy/reference/sparse.html)  \n",
    "<sup>(10)</sup> [Compressed Sparse Row matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix)  \n",
    "<sup>(11)</sup> [SGDClassifier - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)  \n",
    "<sup>(12)</sup> [RandomizedSearchCV - Scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)  \n",
    "<sup>(13)</sup> [Sentiment Classification using Document Embeddings trained with\n",
    "Cosine Similarity](https://www.aclweb.org/anthology/P19-2057.pdf)  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "include_colab_link": true,
   "name": "sentiment-analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
